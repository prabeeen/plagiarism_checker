{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92f4xqycaOiv"
   },
   "source": [
    "**Submitted by: Prabin Sahani**\n",
    "\n",
    "**BECE (Day)**\n",
    "\n",
    "**171342**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LXPhtCTf5ro"
   },
   "source": [
    "**NLTK(Natural Language Toolkit):**\n",
    "\n",
    "  NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning etc.\n",
    "\n",
    "**Scikit-Learn(Sklearn):**\n",
    "\n",
    "  Scikit-learn is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. Here we use Scikit-learn for TfidfVectorizer and cosine_similarity.\n",
    "\n",
    "\n",
    "*The following code imports the necessary library for plagiarism detection.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CXJuX7uLWjMS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MMGheXMmHQuw"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1oeun8XXF2YU",
    "outputId": "7789d110-5d9d-4f04-f44d-059ec74fc3e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTzYAIWLjTeM"
   },
   "source": [
    "*The following code makes a list(student_files) of all the files with .txt file extension.*\n",
    "\n",
    "Note: The files to check plagiarism must be in .txt file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "P6MQ05MeWnOH"
   },
   "outputs": [],
   "source": [
    "student_files = [doc for doc in os.listdir() if doc.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJrpU7pOjqvQ"
   },
   "source": [
    "*The following code reads the content of each file and makes a list(student_notes)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ocwz6ZkIbReE"
   },
   "outputs": [],
   "source": [
    "student_notes =[open(File).read() for File in  student_files]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6npTW2GkLV4"
   },
   "source": [
    "*The following code perform preprocessing of the text in the above list(student_notes). At first, the content of each files are tokenized into sentences with sent_tokenize and then all the numbers, characters such as '.',',','!','?','{',\n",
    "'}','[',']','(',')' and other characters are removed using regular expression.\n",
    "All the uppercases are converted to lowercases and splitted into the words.\n",
    "Then we perform lemmatization of only those words which do not belong to the stopwords of english language.\n",
    "Finally, we join the words into a sentence and append it to the list(student_note)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iPhlNnuaG1fa"
   },
   "outputs": [],
   "source": [
    "wordnet_lem = WordNetLemmatizer()\n",
    "student_note = []\n",
    "for i in student_notes:\n",
    "  sentence = ''\n",
    "  sentences = nltk.sent_tokenize(i)\n",
    "  for j in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[j])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordnet_lem.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    sentence=sentence + review + ' '\n",
    "  student_note.append(sentence.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnJMoebPm8Sa"
   },
   "source": [
    "**Tf-idf:**\n",
    "Term frequency-inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collectionor corpus. It is often used as a weighting factor in searches of information retrieval, text mining and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "**Cosine Similarity:**\n",
    "Cosine similarity is a metric, helpful in determining, how similar the data objects are irrespective of their size. We can measure the similarity between two sentences in python suing cosine similarity. In cosine similarity, data objects in a dataset are treated as a vector. The formula to find the cosine similarity between two bectors is-\n",
    "cos(x,y) = x . y / ||x|| * ||y||\n",
    "\n",
    "*The following lambda function converts the word in sentence of student_note into a vector using TfidfVectorizer() so as to represent the words in numbers which can be provided to the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "b5bG5azxWnjU"
   },
   "outputs": [],
   "source": [
    "vectorize = lambda Text: TfidfVectorizer().fit_transform(Text).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8xHVFqZqDZK"
   },
   "source": [
    "The following lambda function finds the cosine similarity between two text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4NTZV5scWnwV"
   },
   "outputs": [],
   "source": [
    "similarity = lambda doc1, doc2: cosine_similarity([doc1, doc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEttoIc-qNYL"
   },
   "source": [
    "In following code, all the preprocessed texts are converted into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JK4do1VUapzA"
   },
   "outputs": [],
   "source": [
    "vectors = vectorize(student_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mx2IxEy4qZWL"
   },
   "source": [
    "In the following code, the tuples of student_files and vectors are created and strored in s_vectors list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rhzp6AgKap9l"
   },
   "outputs": [],
   "source": [
    "s_vectors = list(zip(student_files, vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CU95FAKQxW8T"
   },
   "source": [
    "In the following code, first we create plagiarism_results which is of set type so as to store unique elements only. Then we find the index of current tuple and remove it so as to get next file and vector tuple. Then we find the similarity between the files, sort the files and provide score of the similarity. Finally, the results are added to the plagiarism_results and is printed after calling the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lExFZuN2aqG0",
    "outputId": "49d45940-2f4e-4f2b-d231-8af6011864f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('source_check.txt', 'test_3.txt', 0.021059745276379792)\n",
      "('source_check.txt', 'student_work.txt', 0.7742890007201874)\n",
      "('student_work.txt', 'test_3.txt', 0.02219225284643004)\n",
      "('New Text Document.txt', 'student_work.txt', 0.0)\n",
      "('New Text Document.txt', 'test_3.txt', 0.0)\n",
      "('New Text Document.txt', 'source_check.txt', 0.0)\n"
     ]
    }
   ],
   "source": [
    "def check_plagiarism():\n",
    "    plagiarism_results = set()\n",
    "    global s_vectors\n",
    "    for student_a, text_vector_a in s_vectors:\n",
    "        new_vectors =s_vectors.copy()\n",
    "        current_index = new_vectors.index((student_a, text_vector_a))\n",
    "        del new_vectors[current_index]\n",
    "        for student_b , text_vector_b in new_vectors:\n",
    "            sim_score = similarity(text_vector_a, text_vector_b)[0][1]\n",
    "            student_pair = sorted((student_a, student_b))\n",
    "            score = (student_pair[0], student_pair[1],sim_score)\n",
    "            plagiarism_results.add(score)\n",
    "    return plagiarism_results\n",
    "    \n",
    "for data in check_plagiarism():\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
